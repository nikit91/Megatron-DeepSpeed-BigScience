cpu-bind=MASK - n2gpu1202, task  0  0 [3837296]: mask |BBBBBBBB|BBBBBBBB||--------|--------||--------|--------||--------|--------||||--------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1202, task  0  0 [3837334]: mask |BBBBBBBB|BBBBBBBB||--------|--------||--------|--------||--------|--------||||--------|--------||--------|--------||--------|--------||--------|--------|  set
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[default0]:using world size: 4, data-parallel-size: 2, tensor-model-parallel size: 2, pipeline-model-parallel size: 1 
[default0]:using torch.float16 for parameters ...
[default0]:------------------------ arguments ------------------------
[default0]:  abort_on_unmet_fused_kernel_constraints ......... False
[default0]:  accumulate_allreduce_grads_in_fp32 .............. False
[default0]:  adam_beta1 ...................................... 0.9
[default0]:  adam_beta2 ...................................... 0.95
[default0]:  adam_eps ........................................ 1e-08
[default0]:  adlr_autoresume ................................. False
[default0]:  adlr_autoresume_interval ........................ 1000
[default0]:  apply_query_key_layer_scaling ................... True
[default0]:  apply_residual_connection_post_layernorm ........ False
[default0]:  attention_dropout ............................... 0.1
[default0]:  attention_softmax_in_fp32 ....................... False
[default0]:  bert_binary_head ................................ True
[default0]:  bert_load ....................................... None
[default0]:  bf16 ............................................ False
[default0]:  bias_dropout_fusion ............................. True
[default0]:  bias_gelu_fusion ................................ True
[default0]:  biencoder_projection_dim ........................ 0
[default0]:  biencoder_shared_query_context_model ............ False
[default0]:  block_data_path ................................. None
[default0]:  checkpoint_activations .......................... True
[default0]:  checkpoint_in_cpu ............................... False
[default0]:  checkpoint_num_layers ........................... 1
[default0]:  clip_grad ....................................... 1.0
[default0]:  codecarbon_dir .................................. None
[default0]:  consumed_train_samples .......................... 0
[default0]:  consumed_train_tokens ........................... 0
[default0]:  consumed_valid_samples .......................... 0
[default0]:  contigious_checkpointing ........................ False
[default0]:  cpu_optimizer ................................... False
[default0]:  cpu_torch_adam .................................. False
[default0]:  curriculum_learning ............................. False
[default0]:  data_impl ....................................... infer
[default0]:  data_parallel_size .............................. 2
[default0]:  data_path ....................................... ['data/meg-gpt2-oscar-en-10k_text_document']
[default0]:  dataloader_type ................................. single
[default0]:  DDP_impl ........................................ local
[default0]:  decoder_seq_length .............................. None
[default0]:  deepscale ....................................... False
[default0]:  deepscale_config ................................ None
[default0]:  deepspeed ....................................... True
[default0]:  deepspeed_activation_checkpointing .............. True
[default0]:  deepspeed_config ................................ ./ds_config.json
[default0]:  deepspeed_mpi ................................... False
[default0]:  distribute_checkpointed_activations ............. False
[default0]:  distributed_backend ............................. nccl
[default0]:  embed_layernorm ................................. False
[default0]:  embedding_path .................................. None
[default0]:  encoder_seq_length .............................. 512
[default0]:  eod_mask_loss ................................... False
[default0]:  eval_interval ................................... 100
[default0]:  eval_iters ...................................... 10
[default0]:  eval_only ....................................... None
[default0]:  evidence_data_path .............................. None
[default0]:  exit_duration_in_mins ........................... None
[default0]:  exit_interval ................................... 100
[default0]:  ffn_hidden_size ................................. 32
[default0]:  finetune ........................................ False
[default0]:  fp16 ............................................ True
[default0]:  fp16_lm_cross_entropy ........................... False
[default0]:  fp32_residual_connection ........................ False
[default0]:  gigaflos_no_embeds .............................. 0
[default0]:  global_batch_size ............................... 16
[default0]:  glu_activation .................................. None
[default0]:  hidden_dropout .................................. 0.1
[default0]:  hidden_size ..................................... 8
[default0]:  hysteresis ...................................... 2
[default0]:  ict_head_size ................................... None
[default0]:  ict_load ........................................ None
[default0]:  img_dim ......................................... 224
[default0]:  indexer_batch_size .............................. 128
[default0]:  indexer_log_interval ............................ 1000
[default0]:  inference ....................................... False
[default0]:  init_method_std ................................. 0.02
[default0]:  init_method_xavier_uniform ...................... False
[default0]:  initial_loss_scale .............................. 4294967296
[default0]:  kill_switch_path ................................ None
[default0]:  kv_channels ..................................... 4
[default0]:  layernorm_epsilon ............................... 1e-05
[default0]:  lazy_mpu_init ................................... None
[default0]:  load ............................................ checkpoints/gpt2-dist
[default0]:  local_rank ...................................... None
[default0]:  log_batch_size_to_tensorboard ................... True
[default0]:  log_interval .................................... 10
[default0]:  log_learning_rate_to_tensorboard ................ True
[default0]:  log_level ....................................... None
[default0]:  log_level_replica ............................... None
[default0]:  log_loss_scale_to_tensorboard ................... True
[default0]:  log_num_zeros_in_grad ........................... False
[default0]:  log_params_norm ................................. False
[default0]:  log_path ........................................ None
[default0]:  log_timers_to_tensorboard ....................... True
[default0]:  log_validation_ppl_to_tensorboard ............... True
[default0]:  loss_on_targets_only ............................ False
[default0]:  loss_scale ...................................... None
[default0]:  loss_scale_window ............................... 1000
[default0]:  lr .............................................. 0.0001
[default0]:  lr_decay_iters .................................. None
[default0]:  lr_decay_samples ................................ 12
[default0]:  lr_decay_style .................................. cosine
[default0]:  lr_decay_tokens ................................. None
[default0]:  lr_warmup_fraction .............................. None
[default0]:  lr_warmup_iters ................................. 0
[default0]:  lr_warmup_samples ............................... 5
[default0]:  make_vocab_size_divisible_by .................... 128
[default0]:  mask_prob ....................................... 0.15
[default0]:  masked_softmax_fusion ........................... True
[default0]:  max_position_embeddings ......................... 512
[default0]:  mean_noise_span_length .......................... None
[default0]:  memory_centric_tiled_linear ..................... False
[default0]:  merge_file ...................................... data/gpt2-merges.txt
[default0]:  micro_batch_size ................................ 1
[default0]:  min_loss_scale .................................. 1.0
[default0]:  min_lr .......................................... 1e-06
[default0]:  mmap_warmup ..................................... False
[default0]:  no_load_optim ................................... None
[default0]:  no_load_rng ..................................... None
[default0]:  no_save_optim ................................... None
[default0]:  no_save_rng ..................................... None
[default0]:  noise_density ................................... None
[default0]:  num_attention_heads ............................. 2
[default0]:  num_channels .................................... 3
[default0]:  num_classes ..................................... 1000
[default0]:  num_layers ...................................... 2
[default0]:  num_layers_per_virtual_pipeline_stage ........... None
[default0]:  num_workers ..................................... 2
[default0]:  onnx_safe ....................................... None
[default0]:  openai_gelu ..................................... False
[default0]:  optimizer ....................................... adam
[default0]:  override_lr_scheduler ........................... False
[default0]:  pad_vocab_size_to ............................... None
[default0]:  params_dtype .................................... torch.float16
[default0]:  partition_activations ........................... True
[default0]:  patch_dim ....................................... 16
[default0]:  pipeline_model_parallel_size .................... 1
[default0]:  position_embedding_type ......................... PositionEmbeddingType.absolute
[default0]:  pp_partition_method ............................. None
[default0]:  profile_backward ................................ False
[default0]:  query_in_block_prob ............................. 0.1
[default0]:  rampup_batch_size ............................... ['2', '2', '1_000']
[default0]:  rank ............................................ 0
[default0]:  remote_device ................................... none
[default0]:  reset_attention_mask ............................ False
[default0]:  reset_position_ids .............................. False
[default0]:  retriever_report_topk_accuracies ................ []
[default0]:  retriever_score_scaling ......................... False
[default0]:  retriever_seq_length ............................ 256
[default0]:  reweight_loss_based_on_position_frequency ....... False
[default0]:  sample_rate ..................................... 1.0
[default0]:  save ............................................ checkpoints/gpt2-dist
[default0]:  save_interval ................................... 50
[default0]:  scatter_gather_tensors_in_pipeline .............. True
[default0]:  scattered_embeddings ............................ False
[default0]:  seed ............................................ 42
[default0]:  seq_length ...................................... 512
[default0]:  sgd_momentum .................................... 0.9
[default0]:  short_seq_prob .................................. 0.1
[default0]:  skip_train_iteration_range ...................... None
[default0]:  split ........................................... 969, 30, 1
[default0]:  split_transformers .............................. False
[default0]:  sync_tp_duplicated_parameters ................... False
[default0]:  synchronize_each_layer .......................... False
[default0]:  tensor_model_parallel_size ...................... 2
[default0]:  tensorboard_dir ................................. output_dir/tensorboard-dist
[default0]:  tensorboard_log_interval ........................ 1
[default0]:  tensorboard_queue_size .......................... 5
[default0]:  test_weighted_split_names ....................... None
[default0]:  test_weighted_split_paths ....................... None
[default0]:  test_weighted_split_paths_path .................. None
[default0]:  test_weighted_split_splits ...................... None
[default0]:  test_weighted_split_weights ..................... None
[default0]:  tile_factor ..................................... 1
[default0]:  titles_data_path ................................ None
[default0]:  tokenizer_name_or_path .......................... None
[default0]:  tokenizer_type .................................. GPT2BPETokenizer
[default0]:  train_iters ..................................... None
[default0]:  train_samples ................................... 10000
[default0]:  train_tokens .................................... None
[default0]:  train_weighted_split_paths ...................... None
[default0]:  train_weighted_split_paths_path ................. None
[default0]:  universal_checkpoint ............................ False
[default0]:  use_bnb_optimizer ............................... False
[default0]:  use_checkpoint_lr_scheduler ..................... False
[default0]:  use_contiguous_buffers_in_ddp ................... False
[default0]:  use_cpu_initialization .......................... None
[default0]:  use_one_sent_docs ............................... False
[default0]:  use_pin_memory .................................. False
[default0]:  valid_num_workers ............................... 2
[default0]:  valid_weighted_split_names ...................... None
[default0]:  valid_weighted_split_paths ...................... None
[default0]:  valid_weighted_split_paths_path ................. None
[default0]:  valid_weighted_split_splits ..................... None
[default0]:  valid_weighted_split_weights .................... None
[default0]:  virtual_pipeline_model_parallel_size ............ None
[default0]:  vocab_extra_ids ................................. 0
[default0]:  vocab_file ...................................... data/gpt2-vocab.json
[default0]:  weight_decay .................................... 0.1
[default0]:  world_size ...................................... 4
[default0]:  zero_allgather_bucket_size ...................... 0.0
[default0]:  zero_contigious_gradients ....................... False
[default0]:  zero_reduce_bucket_size ......................... 0.0
[default0]:  zero_reduce_scatter ............................. False
[default0]:  zero_stage ...................................... 1
[default0]:-------------------- end of arguments ---------------------
[default0]:will use batch size rampup starting from global batch size 2 to global batch size 16 with batch size increments 2 over 1000 samples.
[default0]:> building GPT2BPETokenizer tokenizer ...
[default0]: > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch']
[default0]:torch version .................... 1.13.1
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.8.2+8710f051, 8710f051, master
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.13, cuda 11.7
[default0]:**** Git info for Megatron: git_hash=e52bdab git_branch=main ****
[default0]:> initializing torch distributed ...
[default0]:[2023-02-25 22:06:46,605] [INFO] [comm.py:657:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[default3]:> setting tensorboard ...
[default0]:> initializing tensor model parallel with size 2
[default0]:> initializing pipeline model parallel with size 1
[default0]:> setting random seeds to 42 ...
[default0]:[2023-02-25 22:06:56,987] [INFO] [checkpointing.py:226:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 2760 and data parallel seed: 42
[default0]:> compiling dataset index builder ...
[default0]:make: Entering directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/data'
[default0]:make: Nothing to be done for 'default'.
[default0]:make: Leaving directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/data'
[default0]:>>> done with dataset index builder. Compilation time: 0.374 seconds
[default0]:WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
[default0]:> compiling and loading fused kernels ...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:Loading extension module scaled_masked_softmax_cuda...
[default0]:ninja: no work to do.
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module fused_mix_prec_layer_norm_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:Loading extension module fused_mix_prec_layer_norm_cuda...
[default0]:ninja: no work to do.
[default0]:>>> done with compiling and loading fused kernels. Compilation time: 32.435 seconds
[default0]:time to initialize megatron (seconds): 19.630
[default0]:[after megatron is initialized] datetime: 2023-02-25 22:07:31 
[default0]:building GPT model ...
[default0]:[2023-02-25 22:07:32,123] [INFO] [utils.py:829:see_memory_usage] Before Building Model
[default0]:[2023-02-25 22:07:32,129] [INFO] [utils.py:830:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[default0]:[2023-02-25 22:07:32,130] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 30.91 GB, percent = 6.1%
[default0]:SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
[default0]:Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=0, model=1): 1, ProcessCoord(pipe=0, data=1, model=0): 2, ProcessCoord(pipe=0, data=1, model=1): 3}
[default0]:[2023-02-25 22:07:32,223] [INFO] [module.py:370:_partition_layers] Partitioning pipeline stages with method type:transformer
[default0]:stage=0 layers=9
[default0]:     0: _to_float16
[default0]:     1: EmbeddingPipe
[default0]:     2: <lambda>
[default0]:     3: ParallelTransformerLayerPipe
[default0]:     4: ParallelTransformerLayerPipe
[default0]:     5: undo
[default0]:     6: MixedFusedLayerNorm
[default0]:     7: EmbeddingPipe
[default0]:     8: float16_to_fp32
[default0]:  loss: CrossEntropy
[default0]:[2023-02-25 22:07:33,144] [INFO] [utils.py:829:see_memory_usage] After Building Model
[default0]:[2023-02-25 22:07:33,145] [INFO] [utils.py:830:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[default0]:[2023-02-25 22:07:33,145] [INFO] [utils.py:838:see_memory_usage] CPU Virtual Memory:  used = 30.92 GB, percent = 6.1%
[default0]:setting training iterations to 748
[default0]:> learning rate decay style: cosine
[default0]:DeepSpeed is enabled.
[default0]:[2023-02-25 22:07:33,167] [INFO] [logging.py:75:log_dist] [Rank 0] DeepSpeed info: version=0.8.2+8710f051, git-hash=8710f051, git-branch=master
[default3]:Using /upb/departments/pc2/users/n/nikit/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[default1]:Using /upb/departments/pc2/users/n/nikit/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...
[default1]:Emitting ninja build file /upb/departments/pc2/users/n/nikit/.cache/torch_extensions/py39_cu117/utils/build.ninja...
[default1]:Building extension module utils...
[default1]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:Traceback (most recent call last):
[default0]:  File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/pretrain_gpt.py", line 235, in <module>
[default0]:    main()
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
[default0]:    return f(*args, **kwargs)
[default0]:  File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/pretrain_gpt.py", line 231, in main
[default0]:    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
[default0]:  File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/training.py", line 141, in pretrain
[default0]:    model, optimizer, lr_scheduler = setup_model_and_optimizer(model_provider)
[default0]:  File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/training.py", line 424, in setup_model_and_optimizer
[default0]:    model, optimizer, _, lr_scheduler = deepspeed.initialize(
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/__init__.py", line 138, in initialize
[default0]:    engine = PipelineEngine(args=args,
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/runtime/pipe/engine.py", line 60, in __init__
[default0]:    super().__init__(*super_args, **super_kwargs)
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 287, in __init__
[default0]:    self.monitor = MonitorMaster(self._config.monitor_config)
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/monitor/monitor.py", line 36, in __init__
[default0]:    self.wandb_monitor = WandbMonitor(monitor_config.csv_monitor)
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/monitor/wandb.py", line 14, in __init__
[default0]:    self.group = wandb_config.group
[default0]:AttributeError: 'CSVConfig' object has no attribute 'group'
[default2]:Traceback (most recent call last):
[default2]:  File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/pretrain_gpt.py", line 235, in <module>
[default2]:    main()
[default2]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
[default2]:    return f(*args, **kwargs)
[default2]:  File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/pretrain_gpt.py", line 231, in main
[default2]:    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
[default2]:  File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/training.py", line 141, in pretrain
[default2]:    model, optimizer, lr_scheduler = setup_model_and_optimizer(model_provider)
[default2]:  File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/training.py", line 424, in setup_model_and_optimizer
[default2]:    model, optimizer, _, lr_scheduler = deepspeed.initialize(
[default2]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/__init__.py", line 138, in initialize
[default2]:    engine = PipelineEngine(args=args,
[default2]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/runtime/pipe/engine.py", line 60, in __init__
[default2]:    super().__init__(*super_args, **super_kwargs)
[default2]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 297, in __init__
[default2]:    self._configure_distributed_model(model)
[default2]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1175, in _configure_distributed_model
[default2]:    self._broadcast_model()
[default2]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1098, in _broadcast_model
[default2]:    dist.broadcast(p,
[default2]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/comm/comm.py", line 127, in log_wrapper
[default2]:    return func(*args, **kwargs)
[default2]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/comm/comm.py", line 232, in broadcast
[default2]:    return cdb.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
[default2]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/comm/torch.py", line 70, in broadcast
[default2]:    return torch.distributed.broadcast(tensor=tensor,
[default2]:  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 1404, in broadcast
[default2]:    work = group.broadcast([tensor], opts)
[default2]:RuntimeError: [1] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3837368 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3837370 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3837367) of binary: /scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/bin/python
WARNING:torch.distributed.elastic.multiprocessing.errors.error_handler:torch-elastic-error.json already exists and will be overwritten. Original contents:
{
  "message": {
    "message": "SignalException: Process 1056404 got signal: 15",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n    return f(*args, **kwargs)\n  File \"/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/run.py\", line 762, in main\n    run(args)\n  File \"/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/run.py\", line 753, in run\n    elastic_launch(\n  File \"/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 132, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 237, in launch_agent\n    result = agent.run()\n  File \"/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n    result = f(*args, **kwargs)\n  File \"/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 850, in _invoke_run\n    time.sleep(monitor_interval)\n  File \"/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\ntorch.distributed.elastic.multiprocessing.api.SignalException: Process 1056404 got signal: 15\n",
      "timestamp": "1677358668"
    }
  }
}
Traceback (most recent call last):
  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/run.py", line 766, in <module>
    main()
  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gpt.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-02-25_22:08:01
  host      : n2gpu1202.ab2021.local
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3837369)
  error_file: /tmp/torchelastic_c49uaa86/none_u2mj1v86/attempt_0/2/error.json
  traceback : Traceback (most recent call last):
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
      return f(*args, **kwargs)
    File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/pretrain_gpt.py", line 231, in main
      pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
    File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/training.py", line 141, in pretrain
      model, optimizer, lr_scheduler = setup_model_and_optimizer(model_provider)
    File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/training.py", line 424, in setup_model_and_optimizer
      model, optimizer, _, lr_scheduler = deepspeed.initialize(
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/__init__.py", line 138, in initialize
      engine = PipelineEngine(args=args,
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/runtime/pipe/engine.py", line 60, in __init__
      super().__init__(*super_args, **super_kwargs)
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 297, in __init__
      self._configure_distributed_model(model)
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1175, in _configure_distributed_model
      self._broadcast_model()
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 1098, in _broadcast_model
      dist.broadcast(p,
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/comm/comm.py", line 127, in log_wrapper
      return func(*args, **kwargs)
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/comm/comm.py", line 232, in broadcast
      return cdb.broadcast(tensor=tensor, src=src, group=group, async_op=async_op)
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/comm/torch.py", line 70, in broadcast
      return torch.distributed.broadcast(tensor=tensor,
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 1404, in broadcast
      work = group.broadcast([tensor], opts)
  RuntimeError: [1] is setting up NCCL communicator and retreiving ncclUniqueId from [0] via c10d key-value store by key '0', but store->get('0') got error: Connection reset by peer
  
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-02-25_22:07:56
  host      : n2gpu1202.ab2021.local
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3837367)
  error_file: /tmp/torchelastic_c49uaa86/none_u2mj1v86/attempt_0/0/error.json
  traceback : Traceback (most recent call last):
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
      return f(*args, **kwargs)
    File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/pretrain_gpt.py", line 231, in main
      pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
    File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/training.py", line 141, in pretrain
      model, optimizer, lr_scheduler = setup_model_and_optimizer(model_provider)
    File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/training.py", line 424, in setup_model_and_optimizer
      model, optimizer, _, lr_scheduler = deepspeed.initialize(
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/__init__.py", line 138, in initialize
      engine = PipelineEngine(args=args,
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/runtime/pipe/engine.py", line 60, in __init__
      super().__init__(*super_args, **super_kwargs)
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 287, in __init__
      self.monitor = MonitorMaster(self._config.monitor_config)
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/monitor/monitor.py", line 36, in __init__
      self.wandb_monitor = WandbMonitor(monitor_config.csv_monitor)
    File "/scratch/hpc-prf-lola/lib_repo/conda-libs/envs/lola/lib/python3.9/site-packages/deepspeed/monitor/wandb.py", line 14, in __init__
      self.group = wandb_config.group
  AttributeError: 'CSVConfig' object has no attribute 'group'
  
============================================================
srun: error: n2gpu1202: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=3099036.0
