cpu-bind=MASK - n2gpu1201, task  0  0 [47702]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
LAUNCHER: python -u -m torch.distributed.run --nproc_per_node 1 --nnodes 2 --rdzv_id=1234 --rdzv_endpoint n2gpu1201:6000 --rdzv_backend c10d --max_restarts 0 --tee 3
CMD: pretrain_gpt.py --tensor-model-parallel-size 2 --pipeline-model-parallel-size 1 --distributed-backend nccl --num-layers 2 --hidden-size 8 --num-attention-heads 2 --seq-length 512 --max-position-embeddings 512 --micro-batch-size 1 --rampup-batch-size 2 2 1_000 --global-batch-size 16 --train-samples 10_000 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --adam-eps 1e-8 --lr 1e-4 --lr-warmup-samples 5 --min-lr 1e-6 --lr-decay-style cosine --lr-decay-samples 12 --clip-grad 1.0 --weight-decay 1e-1 --fp16 --partition-activations --seed 42 --vocab-file data/gpt2-vocab.json --merge-file data/gpt2-merges.txt --exit-interval 100 --log-interval 10 --save-interval 50 --eval-interval 100 --eval-iters 10 --checkpoint-activations --save checkpoints/gpt2-dist --load checkpoints/gpt2-dist --data-path data/meg-gpt2-oscar-en-10k_text_document --tensorboard-dir output_dir/tensorboard-dist --tensorboard-queue-size 5 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --deepspeed --zero-stage 1 --deepspeed_config ./ds_config.json --deepspeed-activation-checkpointing
cpu-bind=MASK - n2gpu1202, task  1  0 [36035]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1201, task  0  0 [47808]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
[default0]:pretrain_gpt.py main
[default0]:using world size: 2, data-parallel-size: 1, tensor-model-parallel size: 2, pipeline-model-parallel size: 1 
[default0]:using torch.float16 for parameters ...
[default0]:------------------------ arguments ------------------------
[default0]:  abort_on_unmet_fused_kernel_constraints ......... False
[default0]:  accumulate_allreduce_grads_in_fp32 .............. False
[default0]:  adam_beta1 ...................................... 0.9
[default0]:  adam_beta2 ...................................... 0.95
[default0]:  adam_eps ........................................ 1e-08
[default0]:  adlr_autoresume ................................. False
[default0]:  adlr_autoresume_interval ........................ 1000
[default0]:  apply_query_key_layer_scaling ................... True
[default0]:  apply_residual_connection_post_layernorm ........ False
[default0]:  attention_dropout ............................... 0.1
[default0]:pretrain_gpt.py main
[default0]:  attention_softmax_in_fp32 ....................... False
[default0]:  bert_binary_head ................................ True
[default0]:  bert_load ....................................... None
[default0]:  bf16 ............................................ False
[default0]:  bias_dropout_fusion ............................. True
[default0]:  bias_gelu_fusion ................................ True
[default0]:  biencoder_projection_dim ........................ 0
[default0]:  biencoder_shared_query_context_model ............ False
[default0]:  block_data_path ................................. None
[default0]:  checkpoint_activations .......................... True
[default0]:  checkpoint_in_cpu ............................... False
[default0]:  checkpoint_num_layers ........................... 1
[default0]:  clip_grad ....................................... 1.0
[default0]:  codecarbon_dir .................................. None
[default0]:  consumed_train_samples .......................... 0
[default0]:  consumed_train_tokens ........................... 0
[default0]:  consumed_valid_samples .......................... 0
[default0]:  contigious_checkpointing ........................ False
[default0]:  cpu_optimizer ................................... False
[default0]:  cpu_torch_adam .................................. False
[default0]:  curriculum_learning ............................. False
[default0]:  data_impl ....................................... infer
[default0]:  data_parallel_size .............................. 1
[default0]:  data_path ....................................... ['data/meg-gpt2-oscar-en-10k_text_document']
[default0]:  dataloader_type ................................. single
[default0]:  DDP_impl ........................................ local
[default0]:  decoder_seq_length .............................. None
[default0]:  deepscale ....................................... False
[default0]:  deepscale_config ................................ None
[default0]:  deepspeed ....................................... True
[default0]:  deepspeed_activation_checkpointing .............. True
[default0]:  deepspeed_config ................................ ./ds_config.json
[default0]:  deepspeed_mpi ................................... False
[default0]:  distribute_checkpointed_activations ............. False
[default0]:  distributed_backend ............................. nccl
[default0]:  embed_layernorm ................................. False
[default0]:  embedding_path .................................. None
[default0]:  encoder_seq_length .............................. 512
[default0]:  eod_mask_loss ................................... False
[default0]:  eval_interval ................................... 100
[default0]:  eval_iters ...................................... 10
[default0]:  eval_only ....................................... None
[default0]:  evidence_data_path .............................. None
[default0]:  exit_duration_in_mins ........................... None
[default0]:  exit_interval ................................... 100
[default0]:  ffn_hidden_size ................................. 32
[default0]:  finetune ........................................ False
[default0]:  fp16 ............................................ True
[default0]:  fp16_lm_cross_entropy ........................... False
[default0]:  fp32_residual_connection ........................ False
[default0]:  gigaflos_no_embeds .............................. 0
[default0]:  global_batch_size ............................... 16
[default0]:  glu_activation .................................. None
[default0]:  hidden_dropout .................................. 0.1
[default0]:  hidden_size ..................................... 8
[default0]:  hysteresis ...................................... 2
[default0]:  ict_head_size ................................... None
[default0]:  ict_load ........................................ None
[default0]:  img_dim ......................................... 224
[default0]:  indexer_batch_size .............................. 128
[default0]:  indexer_log_interval ............................ 1000
[default0]:  inference ....................................... False
[default0]:  init_method_std ................................. 0.02
[default0]:  init_method_xavier_uniform ...................... False
[default0]:  initial_loss_scale .............................. 4294967296
[default0]:  kill_switch_path ................................ None
[default0]:  kv_channels ..................................... 4
[default0]:  layernorm_epsilon ............................... 1e-05
[default0]:  lazy_mpu_init ................................... None
[default0]:  load ............................................ checkpoints/gpt2-dist
[default0]:  local_rank ...................................... None
[default0]:  log_batch_size_to_tensorboard ................... True
[default0]:  log_interval .................................... 10
[default0]:  log_learning_rate_to_tensorboard ................ True
[default0]:  log_level ....................................... None
[default0]:  log_level_replica ............................... None
[default0]:  log_loss_scale_to_tensorboard ................... True
[default0]:  log_num_zeros_in_grad ........................... False
[default0]:  log_params_norm ................................. False
[default0]:  log_path ........................................ None
[default0]:  log_timers_to_tensorboard ....................... True
[default0]:  log_validation_ppl_to_tensorboard ............... True
[default0]:  loss_on_targets_only ............................ False
[default0]:  loss_scale ...................................... None
[default0]:  loss_scale_window ............................... 1000
[default0]:  lr .............................................. 0.0001
[default0]:  lr_decay_iters .................................. None
[default0]:  lr_decay_samples ................................ 12
[default0]:  lr_decay_style .................................. cosine
[default0]:  lr_decay_tokens ................................. None
[default0]:  lr_warmup_fraction .............................. None
[default0]:  lr_warmup_iters ................................. 0
[default0]:  lr_warmup_samples ............................... 5
[default0]:  make_vocab_size_divisible_by .................... 128
[default0]:  mask_prob ....................................... 0.15
[default0]:  masked_softmax_fusion ........................... True
[default0]:  max_position_embeddings ......................... 512
[default0]:  mean_noise_span_length .......................... None
[default0]:  memory_centric_tiled_linear ..................... False
[default0]:  merge_file ...................................... data/gpt2-merges.txt
[default0]:  micro_batch_size ................................ 1
[default0]:  min_loss_scale .................................. 1.0
[default0]:  min_lr .......................................... 1e-06
[default0]:  mmap_warmup ..................................... False
[default0]:  no_load_optim ................................... None
[default0]:  no_load_rng ..................................... None
[default0]:  no_save_optim ................................... None
[default0]:  no_save_rng ..................................... None
[default0]:  noise_density ................................... None
[default0]:  num_attention_heads ............................. 2
[default0]:  num_channels .................................... 3
[default0]:  num_classes ..................................... 1000
[default0]:  num_layers ...................................... 2
[default0]:  num_layers_per_virtual_pipeline_stage ........... None
[default0]:  num_workers ..................................... 2
[default0]:  onnx_safe ....................................... None
[default0]:  openai_gelu ..................................... False
[default0]:  optimizer ....................................... adam
[default0]:  override_lr_scheduler ........................... False
[default0]:  pad_vocab_size_to ............................... None
[default0]:  params_dtype .................................... torch.float16
[default0]:  partition_activations ........................... True
[default0]:  patch_dim ....................................... 16
[default0]:  pipeline_model_parallel_size .................... 1
[default0]:  position_embedding_type ......................... PositionEmbeddingType.absolute
[default0]:  pp_partition_method ............................. None
[default0]:  profile_backward ................................ False
[default0]:  query_in_block_prob ............................. 0.1
[default0]:  rampup_batch_size ............................... ['2', '2', '1_000']
[default0]:  rank ............................................ 0
[default0]:  remote_device ................................... none
[default0]:  reset_attention_mask ............................ False
[default0]:  reset_position_ids .............................. False
[default0]:  retriever_report_topk_accuracies ................ []
[default0]:  retriever_score_scaling ......................... False
[default0]:  retriever_seq_length ............................ 256
[default0]:  reweight_loss_based_on_position_frequency ....... False
[default0]:  sample_rate ..................................... 1.0
[default0]:  save ............................................ checkpoints/gpt2-dist
[default0]:  save_interval ................................... 50
[default0]:  scatter_gather_tensors_in_pipeline .............. True
[default0]:  scattered_embeddings ............................ False
[default0]:  seed ............................................ 42
[default0]:  seq_length ...................................... 512
[default0]:  sgd_momentum .................................... 0.9
[default0]:  short_seq_prob .................................. 0.1
[default0]:  skip_train_iteration_range ...................... None
[default0]:  split ........................................... 969, 30, 1
[default0]:  split_transformers .............................. False
[default0]:  sync_tp_duplicated_parameters ................... False
[default0]:  synchronize_each_layer .......................... False
[default0]:  tensor_model_parallel_size ...................... 2
[default0]:  tensorboard_dir ................................. output_dir/tensorboard-dist
[default0]:  tensorboard_log_interval ........................ 1
[default0]:  tensorboard_queue_size .......................... 5
[default0]:  test_weighted_split_names ....................... None
[default0]:  test_weighted_split_paths ....................... None
[default0]:  test_weighted_split_paths_path .................. None
[default0]:  test_weighted_split_splits ...................... None
[default0]:  test_weighted_split_weights ..................... None
[default0]:  tile_factor ..................................... 1
[default0]:  titles_data_path ................................ None
[default0]:  tokenizer_name_or_path .......................... None
[default0]:  tokenizer_type .................................. GPT2BPETokenizer
[default0]:  train_iters ..................................... None
[default0]:  train_samples ................................... 10000
[default0]:  train_tokens .................................... None
[default0]:  train_weighted_split_paths ...................... None
[default0]:  train_weighted_split_paths_path ................. None
[default0]:  universal_checkpoint ............................ False
[default0]:  use_bnb_optimizer ............................... False
[default0]:  use_checkpoint_lr_scheduler ..................... False
[default0]:  use_contiguous_buffers_in_ddp ................... False
[default0]:  use_cpu_initialization .......................... None
[default0]:  use_one_sent_docs ............................... False
[default0]:  use_pin_memory .................................. False
[default0]:  valid_num_workers ............................... 2
[default0]:  valid_weighted_split_names ...................... None
[default0]:  valid_weighted_split_paths ...................... None
[default0]:  valid_weighted_split_paths_path ................. None
[default0]:  valid_weighted_split_splits ..................... None
[default0]:  valid_weighted_split_weights .................... None
[default0]:  virtual_pipeline_model_parallel_size ............ None
[default0]:  vocab_extra_ids ................................. 0
[default0]:  vocab_file ...................................... data/gpt2-vocab.json
[default0]:  weight_decay .................................... 0.1
[default0]:  world_size ...................................... 2
[default0]:  zero_allgather_bucket_size ...................... 0.0
[default0]:  zero_contigious_gradients ....................... False
[default0]:  zero_reduce_bucket_size ......................... 0.0
[default0]:  zero_reduce_scatter ............................. False
[default0]:  zero_stage ...................................... 1
[default0]:-------------------- end of arguments ---------------------
[default0]:will use batch size rampup starting from global batch size 2 to global batch size 16 with batch size increments 2 over 1000 samples.
[default0]:> building GPT2BPETokenizer tokenizer ...
[default0]:WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
[default0]: > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:**** Git info for Megatron: git_hash=e52bdab git_branch=main ****
[default0]:> initializing torch distributed ...
[default0]:[2023-04-19 14:19:01,053] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[default0]:> initializing tensor model parallel with size 2
[default0]:> initializing pipeline model parallel with size 1
[default0]:rank>0 waiting for rank 0 before loading kernels
[default0]:> setting random seeds to 42 ...
[default0]:[2023-04-19 14:19:01,929] [INFO] [checkpointing.py:226:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 2760 and data parallel seed: 42
[default0]:> compiling dataset index builder ...
[default0]:make: Entering directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/data'
[default0]:make: Nothing to be done for 'default'.
[default0]:make: Leaving directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/data'
[default0]:>>> done with dataset index builder. Compilation time: 0.273 seconds
[default0]:WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
[default0]:> compiling and loading fused kernels ...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module scaled_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module fused_mix_prec_layer_norm_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module fused_mix_prec_layer_norm_cuda...
[default0]:rank 0 finished kernel load
[default0]:n2gpu1201:47822:47822 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.69<0>
[default0]:n2gpu1201:47822:47822 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1201:47822:47822 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.69<0>
[default0]:n2gpu1201:47822:47822 [0] NCCL INFO Using network IB
[default0]:NCCL version 2.12.12+cuda11.7
[default0]:n2gpu1202:36052:36052 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.70<0>
[default0]:n2gpu1202:36052:36052 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1202:36052:36052 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.70<0>
[default0]:n2gpu1202:36052:36052 [0] NCCL INFO Using network IB
[default0]:n2gpu1201:47822:47970 [0] NCCL INFO Channel 00/02 :    0   1
[default0]:n2gpu1201:47822:47970 [0] NCCL INFO Channel 01/02 :    0   1
[default0]:n2gpu1201:47822:47970 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[default0]:n2gpu1202:36052:36083 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[default0]:n2gpu1202:36052:36083 [0] NCCL INFO Channel 00/0 : 0[3000] -> 1[3000] [receive] via NET/IB/0
[default0]:n2gpu1201:47822:47970 [0] NCCL INFO Channel 00/0 : 1[3000] -> 0[3000] [receive] via NET/IB/0
[default0]:n2gpu1202:36052:36083 [0] NCCL INFO Channel 01/0 : 0[3000] -> 1[3000] [receive] via NET/IB/0
[default0]:n2gpu1201:47822:47970 [0] NCCL INFO Channel 01/0 : 1[3000] -> 0[3000] [receive] via NET/IB/0
[default0]:n2gpu1201:47822:47970 [0] NCCL INFO Channel 00/0 : 0[3000] -> 1[3000] [send] via NET/IB/0
[default0]:n2gpu1202:36052:36083 [0] NCCL INFO Channel 00/0 : 1[3000] -> 0[3000] [send] via NET/IB/0
[default0]:n2gpu1202:36052:36083 [0] NCCL INFO Channel 01/0 : 1[3000] -> 0[3000] [send] via NET/IB/0
[default0]:n2gpu1201:47822:47970 [0] NCCL INFO Channel 01/0 : 0[3000] -> 1[3000] [send] via NET/IB/0
[default0]:n2gpu1202:36052:36083 [0] NCCL INFO Connected all rings
[default0]:n2gpu1202:36052:36083 [0] NCCL INFO Connected all trees
[default0]:n2gpu1202:36052:36083 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default0]:n2gpu1202:36052:36083 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1202:36052:36083 [0] NCCL INFO comm 0x1475e80090d0 rank 1 nranks 2 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1201:47822:47970 [0] NCCL INFO Connected all rings
[default0]:n2gpu1201:47822:47970 [0] NCCL INFO Connected all trees
[default0]:n2gpu1201:47822:47970 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default0]:n2gpu1201:47822:47970 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[default0]:rank>0 done waiting for rank 0 before loading kernels
[default0]:n2gpu1201:47822:47970 [0] NCCL INFO comm 0x150f680090d0 rank 0 nranks 2 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1201:47822:47822 [0] NCCL INFO Launch mode Parallel
[default0]:barrier after loading kernels
[default0]:barrier after loading kernels
[default0]:>>> done with compiling and loading fused kernels. Compilation time: 4.973 seconds
[default0]:time to initialize megatron (seconds): 59.190
[default0]:[after megatron is initialized] datetime: 2023-04-19 14:19:07 
[default0]:building GPT model ...
[default0]:[2023-04-19 14:19:07,283] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[default0]:[2023-04-19 14:19:07,284] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[default0]:[2023-04-19 14:19:07,284] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 11.76 GB, percent = 2.3%
[default0]:SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
[default0]:Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=0, model=1): 1}
[default0]:NCCL version 2.12.12+cuda11.7
[default0]:[2023-04-19 14:19:07,430] [INFO] [module.py:358:_partition_layers] Partitioning pipeline stages with method type:transformer
[default0]:stage=0 layers=9
[default0]:     0: _to_float16
[default0]:     1: EmbeddingPipe
[default0]:     2: <lambda>
[default0]:     3: ParallelTransformerLayerPipe
[default0]:     4: ParallelTransformerLayerPipe
[default0]:     5: undo
[default0]:     6: MixedFusedLayerNorm
[default0]:     7: EmbeddingPipe
[default0]:     8: float16_to_fp32
[default0]:  loss: CrossEntropy
[default0]:[2023-04-19 14:19:07,469] [INFO] [utils.py:785:see_memory_usage] After Building Model
[default0]:[2023-04-19 14:19:07,470] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[default0]:[2023-04-19 14:19:07,470] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 11.76 GB, percent = 2.3%
[default0]:setting training iterations to 748
[default0]:> learning rate decay style: cosine
[default0]:DeepSpeed is enabled.
[default0]:[2023-04-19 14:19:07,471] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.0, git-hash=unknown, git-branch=unknown
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Connected all rings
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO Connected all trees
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1202:36052:36119 [0] NCCL INFO comm 0x1475b80090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Emitting ninja build file /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117/utils/build.ninja...
[default0]:Building extension module utils...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.135911226272583 seconds
[default0]:Rank: 1 partition count [1, 1, 1] and sizes[(206080, False), (512, False), (168, False)] 
[default0]:wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: Network error (ReadTimeout), entering retry loop.
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: Network error (ReadTimeout), entering retry loop.
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: | Waiting for wandb.init()...
[default0]:wandb: / Waiting for wandb.init()...
[default0]:wandb: - Waiting for wandb.init()...
[default0]:wandb: \ Waiting for wandb.init()...
[default0]:wandb: | Waiting for wandb.init()...
[default0]:Problem at: /scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed/monitor/wandb.py 25 __init__
[default0]:wandb: ERROR Run initialization has timed out after 60.0 sec. 
[default0]:wandb: ERROR Please refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-
[default0]:Traceback (most recent call last):
[default0]:  File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/pretrain_gpt.py", line 236, in <module>
[default0]:    main()
[default0]:  File "/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
[default0]:    return f(*args, **kwargs)
[default0]:  File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/pretrain_gpt.py", line 232, in main
[default0]:    pretrain(train_valid_test_datasets_provider, model_provider, forward_step,
[default0]:  File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/training.py", line 141, in pretrain
[default0]:    model, optimizer, lr_scheduler = setup_model_and_optimizer(model_provider)
[default0]:  File "/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/training.py", line 425, in setup_model_and_optimizer
[default0]:    model, optimizer, _, lr_scheduler = deepspeed.initialize(
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed/__init__.py", line 171, in initialize
[default0]:    engine = PipelineEngine(args=args,
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed/runtime/pipe/engine.py", line 53, in __init__
[default0]:    super().__init__(*super_args, **super_kwargs)
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 276, in __init__
[default0]:    self.monitor = MonitorMaster(self._config.monitor_config)
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed/monitor/monitor.py", line 42, in __init__
[default0]:    self.wandb_monitor = WandbMonitor(monitor_config.wandb)
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed/monitor/wandb.py", line 25, in __init__
[default0]:    wandb.init(project=self.project, group=self.group, entity=self.team)
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1164, in init
[default0]:    raise e
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 1145, in init
[default0]:    run = wi.init()
[default0]:  File "/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/wandb/sdk/wandb_init.py", line 764, in init
[default0]:    raise error
[default0]:wandb.errors.CommError: Run initialization has timed out after 60.0 sec. 
[default0]:Please refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 47822) of binary: /scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/bin/python
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 3398640 ON n2gpu1201 CANCELLED AT 2023-04-19T14:21:22 ***
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 36052 closing signal SIGTERM
slurmstepd: error: *** STEP 3398640.0 ON n2gpu1201 CANCELLED AT 2023-04-19T14:21:22 ***
