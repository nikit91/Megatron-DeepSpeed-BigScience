cpu-bind=MASK - n2gpu1217, task  0  0 [40228]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
LAUNCHER: python -u -m torch.distributed.run --nproc_per_node 4 --nnodes 1 --rdzv_id=1234 --rdzv_endpoint n2gpu1217:6005 --rdzv_backend c10d --max_restarts 0 --tee 3
CMD: pretrain_gpt.py --tensor-model-parallel-size 2 --pipeline-model-parallel-size 1 --distributed-backend nccl --num-layers 24 --hidden-size 1024 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 1 --global-batch-size 64 --train-samples 500000 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --adam-eps 1e-8 --lr 1e-4 --min-lr 1e-6 --lr-decay-style cosine --clip-grad 1.0 --weight-decay 1e-1 --fp16 --partition-activations --seed 42 --vocab-file data/gpt2-vocab.json --merge-file data/gpt2-merges.txt --exit-interval 1000 --log-interval 1 --save-interval 50 --eval-interval 50 --eval-iters 10 --checkpoint-activations --save checkpoints/gpt2-dist --load checkpoints/gpt2-dist --data-path data/meg-gpt2-oscar-en-10k_text_document --tensorboard-dir output_dir/tensorboard-dist --tensorboard-queue-size 5 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --deepspeed --zero-stage 1 --deepspeed_config ./ds_config.json --deepspeed-activation-checkpointing
cpu-bind=MASK - n2gpu1217, task  0  0 [40339]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[default2]:pretrain_gpt.py main
[default0]:pretrain_gpt.py main
[default0]:using world size: 4, data-parallel-size: 2, tensor-model-parallel size: 2, pipeline-model-parallel size: 1 
[default0]:using torch.float16 for parameters ...
[default0]:------------------------ arguments ------------------------
[default0]:  abort_on_unmet_fused_kernel_constraints ......... False
[default0]:  accumulate_allreduce_grads_in_fp32 .............. False
[default0]:  adam_beta1 ...................................... 0.9
[default0]:  adam_beta2 ...................................... 0.95
[default0]:  adam_eps ........................................ 1e-08
[default0]:  adlr_autoresume ................................. False
[default0]:  adlr_autoresume_interval ........................ 1000
[default0]:  apply_query_key_layer_scaling ................... True
[default0]:  apply_residual_connection_post_layernorm ........ False
[default0]:  attention_dropout ............................... 0.1
[default0]:  attention_softmax_in_fp32 ....................... False
[default0]:  bert_binary_head ................................ True
[default0]:  bert_load ....................................... None
[default0]:  bf16 ............................................ False
[default0]:  bias_dropout_fusion ............................. True
[default0]:  bias_gelu_fusion ................................ True
[default0]:  biencoder_projection_dim ........................ 0
[default0]:  biencoder_shared_query_context_model ............ False
[default0]:  block_data_path ................................. None
[default0]:  checkpoint_activations .......................... True
[default0]:  checkpoint_in_cpu ............................... False
[default0]:  checkpoint_num_layers ........................... 1
[default0]:  clip_grad ....................................... 1.0
[default0]:  codecarbon_dir .................................. None
[default0]:  consumed_train_samples .......................... 0
[default0]:  consumed_train_tokens ........................... 0
[default0]:  consumed_valid_samples .......................... 0
[default0]:  contigious_checkpointing ........................ False
[default0]:  cpu_optimizer ................................... False
[default0]:  cpu_torch_adam .................................. False
[default0]:  curriculum_learning ............................. False
[default0]:  data_impl ....................................... infer
[default0]:  data_parallel_size .............................. 2
[default0]:  data_path ....................................... ['data/meg-gpt2-oscar-en-10k_text_document']
[default0]:  dataloader_type ................................. single
[default0]:  DDP_impl ........................................ local
[default0]:  decoder_seq_length .............................. None
[default0]:  deepscale ....................................... False
[default0]:  deepscale_config ................................ None
[default0]:  deepspeed ....................................... True
[default0]:  deepspeed_activation_checkpointing .............. True
[default0]:  deepspeed_config ................................ ./ds_config.json
[default0]:  deepspeed_mpi ................................... False
[default0]:  distribute_checkpointed_activations ............. False
[default0]:  distributed_backend ............................. nccl
[default0]:  embed_layernorm ................................. False
[default0]:  embedding_path .................................. None
[default0]:  encoder_seq_length .............................. 1024
[default0]:  eod_mask_loss ................................... False
[default0]:  eval_interval ................................... 50
[default0]:  eval_iters ...................................... 10
[default0]:  eval_only ....................................... None
[default0]:  evidence_data_path .............................. None
[default0]:  exit_duration_in_mins ........................... None
[default0]:  exit_interval ................................... 1000
[default0]:  ffn_hidden_size ................................. 4096
[default0]:  finetune ........................................ False
[default0]:  fp16 ............................................ True
[default0]:  fp16_lm_cross_entropy ........................... False
[default0]:  fp32_residual_connection ........................ False
[default0]:  gigaflos_no_embeds .............................. 0
[default0]:  global_batch_size ............................... 64
[default0]:  glu_activation .................................. None
[default0]:  hidden_dropout .................................. 0.1
[default0]:  hidden_size ..................................... 1024
[default0]:  hysteresis ...................................... 2
[default0]:  ict_head_size ................................... None
[default0]:  ict_load ........................................ None
[default0]:  img_dim ......................................... 224
[default0]:  indexer_batch_size .............................. 128
[default0]:  indexer_log_interval ............................ 1000
[default0]:  inference ....................................... False
[default0]:  init_method_std ................................. 0.02
[default0]:  init_method_xavier_uniform ...................... False
[default0]:  initial_loss_scale .............................. 4294967296
[default0]:  kill_switch_path ................................ None
[default0]:  kv_channels ..................................... 64
[default0]:  layernorm_epsilon ............................... 1e-05
[default0]:  lazy_mpu_init ................................... None
[default0]:  load ............................................ checkpoints/gpt2-dist
[default0]:  local_rank ...................................... None
[default0]:  log_batch_size_to_tensorboard ................... True
[default0]:  log_interval .................................... 1
[default0]:  log_learning_rate_to_tensorboard ................ True
[default0]:  log_level ....................................... None
[default0]:  log_level_replica ............................... None
[default0]:  log_loss_scale_to_tensorboard ................... True
[default0]:  log_num_zeros_in_grad ........................... False
[default0]:  log_params_norm ................................. False
[default0]:  log_path ........................................ None
[default0]:  log_timers_to_tensorboard ....................... True
[default0]:  log_validation_ppl_to_tensorboard ............... True
[default0]:  loss_on_targets_only ............................ False
[default0]:  loss_scale ...................................... None
[default0]:  loss_scale_window ............................... 1000
[default0]:  lr .............................................. 0.0001
[default0]:  lr_decay_iters .................................. None
[default0]:  lr_decay_samples ................................ None
[default0]:  lr_decay_style .................................. cosine
[default0]:  lr_decay_tokens ................................. None
[default0]:  lr_warmup_fraction .............................. None
[default0]:  lr_warmup_iters ................................. 0
[default0]:  lr_warmup_samples ............................... 0
[default0]:  make_vocab_size_divisible_by .................... 128
[default0]:  mask_prob ....................................... 0.15
[default0]:  masked_softmax_fusion ........................... True
[default0]:  max_position_embeddings ......................... 1024
[default0]:  mean_noise_span_length .......................... None
[default0]:  memory_centric_tiled_linear ..................... False
[default0]:  merge_file ...................................... data/gpt2-merges.txt
[default0]:  micro_batch_size ................................ 1
[default0]:  min_loss_scale .................................. 1.0
[default0]:  min_lr .......................................... 1e-06
[default0]:  mmap_warmup ..................................... False
[default0]:  no_load_optim ................................... None
[default0]:  no_load_rng ..................................... None
[default0]:  no_save_optim ................................... None
[default0]:  no_save_rng ..................................... None
[default0]:  noise_density ................................... None
[default0]:  num_attention_heads ............................. 16
[default0]:  num_channels .................................... 3
[default0]:  num_classes ..................................... 1000
[default0]:  num_layers ...................................... 24
[default0]:  num_layers_per_virtual_pipeline_stage ........... None
[default0]:  num_workers ..................................... 2
[default0]:  onnx_safe ....................................... None
[default0]:  openai_gelu ..................................... False
[default0]:  optimizer ....................................... adam
[default0]:  override_lr_scheduler ........................... False
[default0]:  pad_vocab_size_to ............................... None
[default0]:  params_dtype .................................... torch.float16
[default0]:  partition_activations ........................... True
[default0]:  patch_dim ....................................... 16
[default0]:  pipeline_model_parallel_size .................... 1
[default0]:  position_embedding_type ......................... PositionEmbeddingType.absolute
[default0]:  pp_partition_method ............................. None
[default0]:  profile_backward ................................ False
[default0]:  query_in_block_prob ............................. 0.1
[default0]:  rampup_batch_size ............................... None
[default0]:  rank ............................................ 0
[default0]:  remote_device ................................... none
[default0]:  reset_attention_mask ............................ False
[default0]:  reset_position_ids .............................. False
[default0]:  retriever_report_topk_accuracies ................ []
[default0]:  retriever_score_scaling ......................... False
[default0]:  retriever_seq_length ............................ 256
[default0]:  reweight_loss_based_on_position_frequency ....... False
[default0]:  sample_rate ..................................... 1.0
[default0]:  save ............................................ checkpoints/gpt2-dist
[default0]:  save_interval ................................... 50
[default0]:  scatter_gather_tensors_in_pipeline .............. True
[default0]:  scattered_embeddings ............................ False
[default0]:  seed ............................................ 42
[default0]:  seq_length ...................................... 1024
[default0]:  sgd_momentum .................................... 0.9
[default0]:  short_seq_prob .................................. 0.1
[default0]:  skip_train_iteration_range ...................... None
[default0]:  split ........................................... 969, 30, 1
[default0]:  split_transformers .............................. False
[default0]:  sync_tp_duplicated_parameters ................... False
[default0]:  synchronize_each_layer .......................... False
[default0]:  tensor_model_parallel_size ...................... 2
[default0]:  tensorboard_dir ................................. output_dir/tensorboard-dist
[default0]:  tensorboard_log_interval ........................ 1
[default0]:  tensorboard_queue_size .......................... 5
[default0]:  test_weighted_split_names ....................... None
[default0]:  test_weighted_split_paths ....................... None
[default0]:  test_weighted_split_paths_path .................. None
[default0]:  test_weighted_split_splits ...................... None
[default0]:  test_weighted_split_weights ..................... None
[default0]:  tile_factor ..................................... 1
[default0]:  titles_data_path ................................ None
[default0]:  tokenizer_name_or_path .......................... None
[default0]:  tokenizer_type .................................. GPT2BPETokenizer
[default0]:  train_iters ..................................... None
[default0]:  train_samples ................................... 500000
[default0]:  train_tokens .................................... None
[default0]:  train_weighted_split_paths ...................... None
[default0]:  train_weighted_split_paths_path ................. None
[default0]:  universal_checkpoint ............................ False
[default0]:  use_bnb_optimizer ............................... False
[default0]:  use_checkpoint_lr_scheduler ..................... False
[default0]:  use_contiguous_buffers_in_ddp ................... False
[default0]:  use_cpu_initialization .......................... None
[default0]:  use_one_sent_docs ............................... False
[default0]:  use_pin_memory .................................. False
[default0]:  valid_num_workers ............................... 2
[default0]:  valid_weighted_split_names ...................... None
[default0]:  valid_weighted_split_paths ...................... None
[default0]:  valid_weighted_split_paths_path ................. None
[default0]:  valid_weighted_split_splits ..................... None
[default0]:  valid_weighted_split_weights .................... None
[default0]:  virtual_pipeline_model_parallel_size ............ None
[default0]:  vocab_extra_ids ................................. 0
[default0]:  vocab_file ...................................... data/gpt2-vocab.json
[default0]:  weight_decay .................................... 0.1
[default0]:  world_size ...................................... 4
[default0]:  zero_allgather_bucket_size ...................... 0.0
[default0]:  zero_contigious_gradients ....................... False
[default0]:  zero_reduce_bucket_size ......................... 0.0
[default0]:  zero_reduce_scatter ............................. False
[default0]:  zero_stage ...................................... 1
[default0]:-------------------- end of arguments ---------------------
[default0]:setting number of micro-batches to constant 32
[default0]:> building GPT2BPETokenizer tokenizer ...
[default3]:pretrain_gpt.py main
[default1]:pretrain_gpt.py main
[default0]: > padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default3]:WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
[default0]:**** Git info for Megatron: git_hash=7482fcf git_branch=main ****
[default0]:> initializing torch distributed ...
[default0]:[2023-04-22 20:42:36,116] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[default0]:> initializing tensor model parallel with size 2
[default0]:> initializing pipeline model parallel with size 1
[default2]:rank>0 waiting for rank 0 before loading kernels
[default1]:rank>0 waiting for rank 0 before loading kernels
[default3]:rank>0 waiting for rank 0 before loading kernels
[default0]:> setting random seeds to 42 ...
[default0]:[2023-04-22 20:42:36,748] [INFO] [checkpointing.py:226:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 2760 and data parallel seed: 42
[default0]:> compiling dataset index builder ...
[default0]:make: Entering directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/data'
[default0]:make: Nothing to be done for 'default'.
[default0]:make: Leaving directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/data'
[default0]:>>> done with dataset index builder. Compilation time: 0.889 seconds
[default0]:> compiling and loading fused kernels ...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module scaled_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module fused_mix_prec_layer_norm_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module fused_mix_prec_layer_norm_cuda...
[default0]:rank 0 finished kernel load
[default0]:n2gpu1217:40362:40362 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.85<0>
[default0]:n2gpu1217:40362:40362 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1217:40362:40362 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.85<0>
[default0]:n2gpu1217:40362:40362 [0] NCCL INFO Using network IB
[default0]:NCCL version 2.12.12+cuda11.7
[default2]:n2gpu1217:40364:40364 [2] NCCL INFO Bootstrap : Using ib1:10.10.103.85<0>
[default2]:n2gpu1217:40364:40364 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default3]:n2gpu1217:40365:40365 [3] NCCL INFO Bootstrap : Using ib1:10.10.103.85<0>
[default3]:n2gpu1217:40365:40365 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default2]:n2gpu1217:40364:40364 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.85<0>
[default2]:n2gpu1217:40364:40364 [2] NCCL INFO Using network IB
[default3]:n2gpu1217:40365:40365 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.85<0>
[default3]:n2gpu1217:40365:40365 [3] NCCL INFO Using network IB
[default1]:n2gpu1217:40363:40363 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.85<0>
[default1]:n2gpu1217:40363:40363 [1] NCCL INFO Using network IB
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 00/24 :    0   1   2   3
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 01/24 :    0   1   3   2
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 02/24 :    0   2   3   1
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 03/24 :    0   2   1   3
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 04/24 :    0   3   1   2
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 05/24 :    0   3   2   1
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 06/24 :    0   1   2   3
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 07/24 :    0   1   3   2
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 08/24 :    0   2   3   1
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 09/24 :    0   2   1   3
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 10/24 :    0   3   1   2
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 11/24 :    0   3   2   1
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 12/24 :    0   1   2   3
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 13/24 :    0   1   3   2
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 14/24 :    0   2   3   1
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 15/24 :    0   2   1   3
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 16/24 :    0   3   1   2
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 17/24 :    0   3   2   1
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 18/24 :    0   1   2   3
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 19/24 :    0   1   3   2
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 20/24 :    0   2   3   1
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 21/24 :    0   2   1   3
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 22/24 :    0   3   1   2
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 23/24 :    0   3   2   1
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 2/-1/-1->0->3 [3] 2/-1/-1->0->3 [4] 3/-1/-1->0->2 [5] 3/-1/-1->0->2 [6] -1/-1/-1->0->1 [7] -1/-1/-1->0->1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 2/-1/-1->0->3 [11] 2/-1/-1->0->3 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 2/-1/-1->0->3 [15] 2/-1/-1->0->3 [16] 3/-1/-1->0->2 [17] 3/-1/-1->0->2 [18] -1/-1/-1->0->1 [19] -1/-1/-1->0->1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 2/-1/-1->0->3 [23] 2/-1/-1->0->3
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 3/-1/-1->1->-1 [3] 3/-1/-1->1->-1 [4] -1/-1/-1->1->3 [5] -1/-1/-1->1->3 [6] 0/-1/-1->1->2 [7] 0/-1/-1->1->2 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 3/-1/-1->1->-1 [11] 3/-1/-1->1->-1 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 3/-1/-1->1->-1 [15] 3/-1/-1->1->-1 [16] -1/-1/-1->1->3 [17] -1/-1/-1->1->3 [18] 0/-1/-1->1->2 [19] 0/-1/-1->1->2 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 3/-1/-1->1->-1 [23] 3/-1/-1->1->-1
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] -1/-1/-1->2->0 [3] -1/-1/-1->2->0 [4] 0/-1/-1->2->-1 [5] 0/-1/-1->2->-1 [6] 1/-1/-1->2->3 [7] 1/-1/-1->2->3 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] -1/-1/-1->2->0 [11] -1/-1/-1->2->0 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] -1/-1/-1->2->0 [15] -1/-1/-1->2->0 [16] 0/-1/-1->2->-1 [17] 0/-1/-1->2->-1 [18] 1/-1/-1->2->3 [19] 1/-1/-1->2->3 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] -1/-1/-1->2->0 [23] -1/-1/-1->2->0
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] 0/-1/-1->3->1 [3] 0/-1/-1->3->1 [4] 1/-1/-1->3->0 [5] 1/-1/-1->3->0 [6] 2/-1/-1->3->-1 [7] 2/-1/-1->3->-1 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] 0/-1/-1->3->1 [11] 0/-1/-1->3->1 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] 0/-1/-1->3->1 [15] 0/-1/-1->3->1 [16] 1/-1/-1->3->0 [17] 1/-1/-1->3->0 [18] 2/-1/-1->3->-1 [19] 2/-1/-1->3->-1 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] 0/-1/-1->3->1 [23] 0/-1/-1->3->1
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 00 : 1[44000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 00 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 00 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 04 : 1[44000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 01 : 0[3000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 02 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 00 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 06 : 1[44000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 06 : 0[3000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 06 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 03 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 07 : 0[3000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 06 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 10 : 1[44000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 08 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 12 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 12 : 1[44000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 12 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 14 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 09 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 13 : 0[3000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 12 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 16 : 1[44000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 18 : 0[3000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 18 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 18 : 1[44000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 20 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 15 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 19 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 22 : 1[44000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 18 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 21 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 01 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 03 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 02 : 0[3000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 07 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 02 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 09 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 01 : 2[84000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 03 : 0[3000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 04 : 2[84000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 04 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 13 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 07 : 2[84000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 08 : 0[3000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 08 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 09 : 0[3000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 15 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 10 : 2[84000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 19 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 21 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 14 : 0[3000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 10 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 13 : 2[84000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 14 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 15 : 0[3000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 16 : 2[84000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 19 : 2[84000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 16 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 20 : 0[3000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 20 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 21 : 0[3000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 22 : 2[84000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 22 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 03 : 2[84000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 04 : 0[3000] -> 3[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 01 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 05 : 0[3000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 02 : 1[44000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 05 : 1[44000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 05 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 05 : 2[84000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 09 : 2[84000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 10 : 0[3000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 08 : 1[44000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 07 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 11 : 0[3000] -> 3[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 11 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 11 : 2[84000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 15 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 11 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 16 : 0[3000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 14 : 1[44000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 13 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 17 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 17 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 17 : 0[3000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 22 : 0[3000] -> 3[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 17 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 21 : 2[84000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 23 : 2[84000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 19 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 20 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 23 : 0[3000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 23 : 1[44000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 23 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Connected all rings
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Connected all rings
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Connected all rings
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Connected all rings
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 01 : 1[44000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 07 : 1[44000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 08 : 1[44000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 09 : 1[44000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 02 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 13 : 1[44000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 04 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 19 : 1[44000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 05 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 01 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 07 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 09 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 10 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 13 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 20 : 1[44000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 19 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 08 : 0[3000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 11 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 21 : 1[44000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 09 : 0[3000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 14 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 21 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 20 : 0[3000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 16 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 21 : 0[3000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 17 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 22 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 23 : 3[c4000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 02 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 04 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 05 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 10 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 11 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 02 : 2[84000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 03 : 2[84000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 14 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 16 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 04 : 0[3000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 05 : 0[3000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 03 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 05 : 2[84000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 11 : 2[84000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 17 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 10 : 0[3000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 05 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 22 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 11 : 0[3000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 14 : 2[84000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 11 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 15 : 2[84000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 23 : 1[44000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 16 : 0[3000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 15 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 17 : 0[3000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 17 : 2[84000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 22 : 0[3000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 17 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 23 : 2[84000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 23 : 3[c4000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 23 : 0[3000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 00 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 06 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 08 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 09 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 12 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 00 : 2[84000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 18 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 00 : 1[44000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 20 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 01 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 01 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 02 : 0[3000] -> 3[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Channel 21 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 06 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 06 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 03 : 0[3000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 14 : 0[3000] -> 3[c4000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 07 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 07 : 1[44000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 08 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 09 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Channel 15 : 0[3000] -> 3[c4000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 12 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 12 : 1[44000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 13 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 13 : 1[44000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 18 : 2[84000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 19 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 18 : 1[44000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 19 : 1[44000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Channel 20 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Channel 21 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO Connected all trees
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO 24 coll channels, 32 p2p channels, 8 p2p channels per peer
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO Connected all trees
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO Connected all trees
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO 24 coll channels, 32 p2p channels, 8 p2p channels per peer
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO Connected all trees
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO 24 coll channels, 32 p2p channels, 8 p2p channels per peer
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO 24 coll channels, 32 p2p channels, 8 p2p channels per peer
[default0]:n2gpu1217:40362:40551 [0] NCCL INFO comm 0x1506a80090d0 rank 0 nranks 4 cudaDev 0 busId 3000 - Init COMPLETE
[default2]:n2gpu1217:40364:40555 [2] NCCL INFO comm 0x148efc0090d0 rank 2 nranks 4 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1217:40365:40557 [3] NCCL INFO comm 0x1493cc0090d0 rank 3 nranks 4 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1217:40363:40560 [1] NCCL INFO comm 0x1495e00090d0 rank 1 nranks 4 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:rank>0 done waiting for rank 0 before loading kernels
[default0]:n2gpu1217:40362:40362 [0] NCCL INFO Launch mode Parallel
[default0]:barrier after loading kernels
[default2]:rank>0 done waiting for rank 0 before loading kernels
[default3]:rank>0 done waiting for rank 0 before loading kernels
[default1]:barrier after loading kernels
[default3]:barrier after loading kernels
[default2]:barrier after loading kernels
[default0]:>>> done with compiling and loading fused kernels. Compilation time: 31.910 seconds
[default0]:time to initialize megatron (seconds): 61.549
[default0]:[after megatron is initialized] datetime: 2023-04-22 20:43:09 
[default0]:building GPT model ...
[default0]:[2023-04-22 20:43:09,644] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[default0]:[2023-04-22 20:43:09,645] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[default0]:[2023-04-22 20:43:09,645] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 15.85 GB, percent = 3.1%
[default0]:SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
[default0]:Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=0, model=1): 1, ProcessCoord(pipe=0, data=1, model=0): 2, ProcessCoord(pipe=0, data=1, model=1): 3}
[default0]:[2023-04-22 20:43:09,832] [INFO] [module.py:358:_partition_layers] Partitioning pipeline stages with method type:transformer
[default0]:stage=0 layers=31
[default0]:     0: _to_float16
[default0]:     1: EmbeddingPipe
[default0]:     2: <lambda>
[default0]:     3: ParallelTransformerLayerPipe
[default0]:     4: ParallelTransformerLayerPipe
[default0]:     5: ParallelTransformerLayerPipe
[default0]:     6: ParallelTransformerLayerPipe
[default0]:     7: ParallelTransformerLayerPipe
[default0]:     8: ParallelTransformerLayerPipe
[default0]:     9: ParallelTransformerLayerPipe
[default0]:    10: ParallelTransformerLayerPipe
[default0]:    11: ParallelTransformerLayerPipe
[default0]:    12: ParallelTransformerLayerPipe
[default0]:    13: ParallelTransformerLayerPipe
[default0]:    14: ParallelTransformerLayerPipe
[default0]:    15: ParallelTransformerLayerPipe
[default0]:    16: ParallelTransformerLayerPipe
[default0]:    17: ParallelTransformerLayerPipe
[default0]:    18: ParallelTransformerLayerPipe
[default0]:    19: ParallelTransformerLayerPipe
[default0]:    20: ParallelTransformerLayerPipe
[default0]:    21: ParallelTransformerLayerPipe
[default0]:    22: ParallelTransformerLayerPipe
[default0]:    23: ParallelTransformerLayerPipe
[default0]:    24: ParallelTransformerLayerPipe
[default0]:    25: ParallelTransformerLayerPipe
[default0]:    26: ParallelTransformerLayerPipe
[default0]:    27: undo
[default0]:    28: MixedFusedLayerNorm
[default0]:    29: EmbeddingPipe
[default0]:    30: float16_to_fp32
[default0]:  loss: CrossEntropy
[default1]:NCCL version 2.12.12+cuda11.7
[default0]:[2023-04-22 20:43:10,163] [INFO] [utils.py:785:see_memory_usage] After Building Model
[default0]:[2023-04-22 20:43:10,164] [INFO] [utils.py:786:see_memory_usage] MA 0.34 GB         Max_MA 0.34 GB         CA 0.37 GB         Max_CA 0 GB 
[default0]:[2023-04-22 20:43:10,164] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 15.89 GB, percent = 3.2%
[default0]:setting training iterations to 7812
[default0]:> learning rate decay style: cosine
[default0]:DeepSpeed is enabled.
[default0]:[2023-04-22 20:43:10,166] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.0, git-hash=unknown, git-branch=unknown
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 00/08 :    0   1
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 01/08 :    0   1
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 02/08 :    0   1
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 03/08 :    0   1
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 04/08 :    0   1
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 05/08 :    0   1
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 06/08 :    0   1
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 07/08 :    0   1
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 00/08 :    0   1
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 01/08 :    0   1
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 02/08 :    0   1
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 03/08 :    0   1
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 04/08 :    0   1
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 05/08 :    0   1
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 06/08 :    0   1
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 07/08 :    0   1
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 00 : 0[44000] -> 1[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 01 : 0[44000] -> 1[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO Channel 00 : 1[c4000] -> 0[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 02 : 0[44000] -> 1[c4000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO Channel 00 : 1[84000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO Channel 01 : 1[c4000] -> 0[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 03 : 0[44000] -> 1[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO Channel 02 : 1[c4000] -> 0[44000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 00 : 0[3000] -> 1[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO Channel 01 : 1[84000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 04 : 0[44000] -> 1[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO Channel 03 : 1[c4000] -> 0[44000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 01 : 0[3000] -> 1[84000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 05 : 0[44000] -> 1[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 02 : 0[3000] -> 1[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO Channel 02 : 1[84000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 06 : 0[44000] -> 1[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO Channel 04 : 1[c4000] -> 0[44000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO Channel 05 : 1[c4000] -> 0[44000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 03 : 0[3000] -> 1[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO Channel 03 : 1[84000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Channel 07 : 0[44000] -> 1[c4000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO Channel 04 : 1[84000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO Channel 06 : 1[c4000] -> 0[44000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 04 : 0[3000] -> 1[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 05 : 0[3000] -> 1[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO Channel 05 : 1[84000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO Channel 07 : 1[c4000] -> 0[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO Channel 06 : 1[84000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 06 : 0[3000] -> 1[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO Channel 07 : 1[84000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Connected all rings
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO Connected all trees
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO Connected all rings
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO Connected all trees
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Channel 07 : 0[3000] -> 1[84000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40636 [1] NCCL INFO comm 0x1495440090d0 rank 0 nranks 2 cudaDev 1 busId 44000 - Init COMPLETE
[default3]:n2gpu1217:40365:40637 [3] NCCL INFO comm 0x14933c0090d0 rank 1 nranks 2 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Connected all rings
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO Connected all trees
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default1]:n2gpu1217:40363:40363 [1] NCCL INFO Launch mode Parallel
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO Connected all rings
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO Connected all trees
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default2]:n2gpu1217:40364:40639 [2] NCCL INFO comm 0x148e6c0090d0 rank 1 nranks 2 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1217:40362:40640 [0] NCCL INFO comm 0x1506140090d0 rank 0 nranks 2 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1217:40362:40362 [0] NCCL INFO Launch mode Parallel
[default0]:[2023-04-22 20:43:15,514] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[default0]:[2023-04-22 20:43:15,538] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[default0]:[2023-04-22 20:43:15,538] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[default0]:[2023-04-22 20:43:15,546] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[default0]:[2023-04-22 20:43:15,546] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[default0]:[2023-04-22 20:43:15,546] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer
[default0]:[2023-04-22 20:43:15,546] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[default0]:[2023-04-22 20:43:15,546] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[default0]:[2023-04-22 20:43:15,546] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[default0]:[2023-04-22 20:43:15,546] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Emitting ninja build file /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117/utils/build.ninja...
[default3]:Building extension module utils...
[default3]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default3]:ninja: no work to do.
[default3]:Loading extension module utils...
[default0]:Loading extension module utils...
[default3]:Time to load utils op: 0.19933295249938965 seconds
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.2465672492980957 seconds
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.23628973960876465 seconds
[default0]:Time to load utils op: 0.23199748992919922 seconds
[default3]:Rank: 3 partition count [2, 2, 2] and sizes[(50135040, False), (38797312, False), (117760, False)] 
[default1]:Rank: 1 partition count [2, 2, 2] and sizes[(50135040, False), (38797312, False), (117760, False)] 
[default0]:Rank: 0 partition count [2, 2, 2] and sizes[(50135040, False), (38797312, False), (117760, False)] 
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Rank: 2 partition count [2, 2, 2] and sizes[(50135040, False), (38797312, False), (117760, False)] 
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:No modifications detected for re-loaded extension module utils, skipping build step...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.02509617805480957 seconds
[default0]:[2023-04-22 20:43:16,856] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[default0]:[2023-04-22 20:43:16,857] [INFO] [utils.py:786:see_memory_usage] MA 0.66 GB         Max_MA 0.67 GB         CA 0.67 GB         Max_CA 1 GB 
[default0]:[2023-04-22 20:43:16,857] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.38 GB, percent = 3.3%
[default3]:No modifications detected for re-loaded extension module utils, skipping build step...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.024382591247558594 seconds
[default2]:No modifications detected for re-loaded extension module utils, skipping build step...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.019882917404174805 seconds
[default2]:NCCL version 2.12.12+cuda11.7
[default0]:[2023-04-22 20:43:17,020] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[default0]:[2023-04-22 20:43:17,021] [INFO] [utils.py:786:see_memory_usage] MA 1.33 GB         Max_MA 1.66 GB         CA 1.66 GB         Max_CA 2 GB 
[default0]:[2023-04-22 20:43:17,021] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.38 GB, percent = 3.3%
[default0]:[2023-04-22 20:43:17,021] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[default0]:[2023-04-22 20:43:17,065] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[default0]:[2023-04-22 20:43:17,065] [INFO] [utils.py:786:see_memory_usage] MA 1.33 GB         Max_MA 1.33 GB         CA 1.66 GB         Max_CA 2 GB 
[default0]:[2023-04-22 20:43:17,066] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.38 GB, percent = 3.3%
[default0]:[2023-04-22 20:43:17,145] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[default0]:[2023-04-22 20:43:17,145] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[default0]:[2023-04-22 20:43:17,145] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x1507317cf160>
[default0]:[2023-04-22 20:43:17,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-04-22 20:43:17,146] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[default0]:[2023-04-22 20:43:17,146] [INFO] [config.py:957:print]   activation_checkpointing_config  {
[default0]:    "partition_activations": false, 
[default0]:    "contiguous_memory_optimization": false, 
[default0]:    "cpu_checkpointing": false, 
[default0]:    "number_checkpoints": null, 
[default0]:    "synchronize_checkpoint_boundary": false, 
[default0]:    "profile": false
[default0]:}
[default0]:[2023-04-22 20:43:17,146] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[default0]:[2023-04-22 20:43:17,146] [INFO] [config.py:957:print]   amp_enabled .................. False
[default0]:[2023-04-22 20:43:17,146] [INFO] [config.py:957:print]   amp_params ................... False
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   autotuning_config ............ {
[default0]:    "enabled": false, 
[default0]:    "start_step": null, 
[default0]:    "end_step": null, 
[default0]:    "metric_path": null, 
[default0]:    "arg_mappings": null, 
[default0]:    "metric": "throughput", 
[default0]:    "model_info": null, 
[default0]:    "results_dir": "autotuning_results", 
[default0]:    "exps_dir": "autotuning_exps", 
[default0]:    "overwrite": true, 
[default0]:    "fast": true, 
[default0]:    "start_profile_step": 3, 
[default0]:    "end_profile_step": 5, 
[default0]:    "tuner_type": "gridsearch", 
[default0]:    "tuner_early_stopping": 5, 
[default0]:    "tuner_num_trials": 50, 
[default0]:    "model_info_path": null, 
[default0]:    "mp_size": 1, 
[default0]:    "max_train_batch_size": null, 
[default0]:    "min_train_batch_size": 1, 
[default0]:    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[default0]:    "min_train_micro_batch_size_per_gpu": 1, 
[default0]:    "num_tuning_micro_batch_sizes": 3
[default0]:}
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x150731305ff0>
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   communication_data_type ...... None
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   disable_allgather ............ False
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   dump_state ................... False
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[default0]:[2023-04-22 20:43:17,147] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   flops_profiler_config ........ {
[default0]:    "enabled": false, 
[default0]:    "profile_step": 1, 
[default0]:    "module_depth": -1, 
[default0]:    "top_modules": 1, 
[default0]:    "detailed": true, 
[default0]:    "output_file": null
[default0]:}
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   fp16_auto_cast ............... False
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   fp16_enabled ................. True
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   global_rank .................. 0
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 32
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[default0]:[2023-04-22 20:43:17,148] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[default0]:[2023-04-22 20:43:17,266] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[default0]:[2023-04-22 20:43:17,307] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 4096
[default0]:[2023-04-22 20:43:17,307] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[default0]:[2023-04-22 20:43:17,307] [INFO] [config.py:957:print]   loss_scale ................... 0
[default0]:[2023-04-22 20:43:17,307] [INFO] [config.py:957:print]   memory_breakdown ............. False
[default0]:[2023-04-22 20:43:17,307] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[default0]:[2023-04-22 20:43:17,307] [INFO] [config.py:957:print]   nebula_config ................ {
[default0]:    "enabled": false, 
[default0]:    "persistent_storage_path": null, 
[default0]:    "persistent_time_interval": 100, 
[default0]:    "num_of_version_in_retention": 2, 
[default0]:    "enable_nebula_load": true, 
[default0]:    "load_path": null
[default0]:}
[default0]:[2023-04-22 20:43:17,307] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[default0]:[2023-04-22 20:43:17,307] [INFO] [config.py:957:print]   optimizer_name ............... None
[default0]:[2023-04-22 20:43:17,307] [INFO] [config.py:957:print]   optimizer_params ............. None
[default0]:[2023-04-22 20:43:17,307] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   pld_enabled .................. False
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   pld_params ................... False
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   prescale_gradients ........... False
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   scheduler_name ............... None
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   scheduler_params ............. None
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   sparse_attention ............. None
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   steps_per_print .............. 2000
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   train_batch_size ............. 64
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   world_size ................... 2
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   zero_enabled ................. True
[default0]:[2023-04-22 20:43:17,308] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[default0]:[2023-04-22 20:43:17,309] [INFO] [config.py:957:print]   zero_optimization_stage ...... 1
[default0]:[2023-04-22 20:43:17,309] [INFO] [config.py:943:print_user_config]   json = {
[default0]:    "train_micro_batch_size_per_gpu": 1, 
[default0]:    "train_batch_size": 64, 
[default0]:    "gradient_clipping": 1, 
[default0]:    "zero_optimization": {
[default0]:        "stage": 1
[default0]:    }, 
[default0]:    "fp16": {
[default0]:        "enabled": true, 
[default0]:        "loss_scale": 0, 
[default0]:        "loss_scale_window": 500, 
[default0]:        "hysteresis": 2, 
[default0]:        "min_loss_scale": 1, 
[default0]:        "initial_scale_power": 12
[default0]:    }, 
[default0]:    "steps_per_print": 2.000000e+03, 
[default0]:    "wall_clock_breakdown": false
[default0]:}
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:No modifications detected for re-loaded extension module utils, skipping build step...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.022221088409423828 seconds
[default0]:[2023-04-22 20:43:17,331] [INFO] [engine.py:81:__init__] CONFIG: micro_batches=32 micro_batch_size=1
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 00/08 :    0   1
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 01/08 :    0   1
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 02/08 :    0   1
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 03/08 :    0   1
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 04/08 :    0   1
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 05/08 :    0   1
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 06/08 :    0   1
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 07/08 :    0   1
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 00/08 :    0   1
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 01/08 :    0   1
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 02/08 :    0   1
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 03/08 :    0   1
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 04/08 :    0   1
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 05/08 :    0   1
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 06/08 :    0   1
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 07/08 :    0   1
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO Channel 00 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 00 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO Channel 01 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 01 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO Channel 02 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 02 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO Channel 03 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO Channel 04 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 03 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO Channel 00 : 1[44000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 04 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 00 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO Channel 01 : 1[44000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO Channel 02 : 1[44000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO Channel 05 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 01 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO Channel 03 : 1[44000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO Channel 06 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 05 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 02 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO Channel 04 : 1[44000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 06 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 03 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO Channel 05 : 1[44000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO Channel 07 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 04 : 0[3000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Channel 07 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO Channel 06 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 05 : 0[3000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO Connected all rings
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO Connected all trees
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Connected all rings
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO Connected all trees
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO Channel 07 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 06 : 0[3000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Channel 07 : 0[3000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40660 [3] NCCL INFO comm 0x1492b40090d0 rank 1 nranks 2 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1217:40364:40659 [2] NCCL INFO comm 0x148de80090d0 rank 0 nranks 2 cudaDev 2 busId 84000 - Init COMPLETE
[default2]:n2gpu1217:40364:40364 [2] NCCL INFO Launch mode Parallel
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Connected all rings
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO Connected all trees
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO Connected all rings
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO Connected all trees
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default1]:n2gpu1217:40363:40665 [1] NCCL INFO comm 0x1494c40090d0 rank 1 nranks 2 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:[2023-04-22 20:43:20,962] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:n2gpu1217:40362:40664 [0] NCCL INFO comm 0x1505980090d0 rank 0 nranks 2 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1217:40362:40362 [0] NCCL INFO Launch mode Parallel
[default0]:[2023-04-22 20:43:20,897] [INFO] [engine.py:136:__init__] RANK=0 STAGE=0 LAYERS=31 [0, 31) STAGE_PARAMS=178100224 (178.100M) TOTAL_PARAMS=356200448 (356.200M) UNIQUE_PARAMS=356200448 (356.200M)
[default3]:[2023-04-22 20:43:20,962] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:time (ms) | load-checkpoint: 46.25
[default1]:[2023-04-22 20:43:20,950] [INFO] [engine.py:136:__init__] RANK=1 STAGE=0 LAYERS=31 [0, 31) STAGE_PARAMS=178100224 (178.100M) TOTAL_PARAMS=356200448 (356.200M) UNIQUE_PARAMS=356200448 (356.200M)
[default1]:[2023-04-22 20:43:20,951] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2023-04-22 20:43:20,982] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:WARNING: could not find the metadata file checkpoints/gpt2-dist 
[default0]:    will not load any checkpoints and will start from random
[default0]:estimated model parameters: 0.356200448
[default0]:estimated model parameters without embeddings: 0.302460928
[default0]:[after model, optimizer, and learning rate scheduler are built] datetime: 2023-04-22 20:43:21 
[default0]:> building train, validation, and test datasets ...
[default0]: > datasets target sizes (minimum size):
[default0]:    train:      500000
[default0]:    validation: 100480
[default0]:    test:       640
[default0]:> building train, validation, and test datasets for GPT ...
[default0]: > building dataset index ...
[default0]:    reading sizes...
[default0]:    reading pointers...
[default0]:    reading document index...
[default0]:    creating numpy buffer of mmap...
[default0]:    creating memory view of numpy buffer...
[default0]: > finished creating indexed dataset in 0.025237 seconds
[default0]:    number of documents: 10000
[default0]: > dataset split:
[default0]:    train:
[default0]:     document indices in [0, 9690) total of 9690 documents
[default0]:    validation:
[default0]:     document indices in [9690, 9990) total of 300 documents
[default0]:    test:
[default0]:     document indices in [9990, 10000) total of 10 documents
[default0]: > WARNING: could not find index map files, building the indices on rank 0 ...
[default0]: > last epoch number of samples (21154) is smaller than 95.0% of number of samples per epoch (28167), setting separate_last_epoch to True
[default0]: > elasped time to build and save doc-idx mapping (seconds): 0.009452
[default0]:    using:
[default0]:     number of documents:       9690
[default0]:     number of epochs:          18
[default0]:     sequence length:           1024
[default0]:     total number of samples:   507013
[default0]: > elasped time to build and save sample-idx mapping (seconds): 0.018965
[default0]: > building shuffle index with split [0, 478846) and [478846, 507013) ...
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-BigScience/megatron/utils.py:349: UserWarning: Parameter count with the embeddings will be inaccurate with PP > 1, as the first and last stage hold several copies of the embeddings
[default0]:  warnings.warn("Parameter count with the embeddings will be inaccurate with PP > 1, as the first and last stage hold several copies of the embeddings")
[default0]: > elasped time to build and save shuffle-idx mapping (seconds): 0.011159
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 00/08 :    0   1
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 01/08 :    0   1
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 02/08 :    0   1
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 03/08 :    0   1
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 04/08 :    0   1
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 05/08 :    0   1
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 06/08 :    0   1
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 07/08 :    0   1
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 00 : 0[3000] -> 1[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO Channel 00 : 1[84000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 01 : 0[3000] -> 1[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO Channel 01 : 1[84000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 02 : 0[3000] -> 1[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 03 : 0[3000] -> 1[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO Channel 02 : 1[84000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 04 : 0[3000] -> 1[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO Channel 03 : 1[84000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 05 : 0[3000] -> 1[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO Channel 04 : 1[84000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO Channel 05 : 1[84000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 06 : 0[3000] -> 1[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Channel 07 : 0[3000] -> 1[84000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO Channel 06 : 1[84000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO Channel 07 : 1[84000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO Connected all rings
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO Connected all trees
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Connected all rings
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO Connected all trees
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default2]:n2gpu1217:40364:40674 [2] NCCL INFO comm 0x148ddc0090d0 rank 1 nranks 2 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1217:40362:40673 [0] NCCL INFO comm 0x1505983211a0 rank 0 nranks 2 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1217:40362:40362 [0] NCCL INFO Launch mode Parallel
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Connected all rings
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO Connected all trees
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Connected all rings
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO Connected all trees
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1217:40364:40682 [2] NCCL INFO comm 0x148dd40090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1217:40362:40680 [0] NCCL INFO comm 0x1505840090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default0]: > loading doc-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_train_indexmap_500000ns_1024sl_42s_doc_idx.npy
[default0]: > loading sample-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_train_indexmap_500000ns_1024sl_42s_sample_idx.npy
[default0]: > loading shuffle-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_train_indexmap_500000ns_1024sl_42s_shuffle_idx.npy
[default0]:    loaded indexed file in 0.005 seconds
[default0]:    total number of samples: 507014
[default0]:    total number of epochs: 18
[default0]: > WARNING: could not find index map files, building the indices on rank 0 ...
[default0]: > last epoch number of samples (748) is smaller than 95.0% of number of samples per epoch (914), setting separate_last_epoch to True
[default0]: > elasped time to build and save doc-idx mapping (seconds): 0.061084
[default0]:    using:
[default0]:     number of documents:       300
[default0]:     number of epochs:          110
[default0]:     sequence length:           1024
[default0]:     total number of samples:   100647
[default0]: > elasped time to build and save sample-idx mapping (seconds): 0.041623
[default0]: > building shuffle index with split [0, 99732) and [99732, 100647) ...
[default0]: > elasped time to build and save shuffle-idx mapping (seconds): 0.057917
[default0]: > loading doc-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_valid_indexmap_100480ns_1024sl_42s_doc_idx.npy
[default0]: > loading sample-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_valid_indexmap_100480ns_1024sl_42s_sample_idx.npy
[default0]: > loading shuffle-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_valid_indexmap_100480ns_1024sl_42s_shuffle_idx.npy
[default0]:    loaded indexed file in 0.002 seconds
[default0]:    total number of samples: 100648
[default0]:    total number of epochs: 110
[default0]: > WARNING: could not find index map files, building the indices on rank 0 ...
[default0]: > last epoch number of samples (5) is smaller than 95.0% of number of samples per epoch (31), setting separate_last_epoch to True
[default0]: > elasped time to build and save doc-idx mapping (seconds): 0.052798
[default0]:    using:
[default0]:     number of documents:       10
[default0]:     number of epochs:          21
[default0]:     sequence length:           1024
[default0]:     total number of samples:   667
[default0]: > elasped time to build and save sample-idx mapping (seconds): 0.056924
[default0]: > building shuffle index with split [0, 635) and [635, 667) ...
[default0]: > elasped time to build and save shuffle-idx mapping (seconds): 0.050910
[default0]: > loading doc-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_test_indexmap_640ns_1024sl_42s_doc_idx.npy
[default0]: > loading sample-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_test_indexmap_640ns_1024sl_42s_sample_idx.npy
[default0]: > loading shuffle-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_test_indexmap_640ns_1024sl_42s_shuffle_idx.npy
[default0]:    loaded indexed file in 0.002 seconds
[default0]:    total number of samples: 668
[default0]:    total number of epochs: 21
[default0]:> finished creating GPT datasets ...
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default0]:  warnings.warn(_create_warning_msg(
[default2]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default2]:  warnings.warn(_create_warning_msg(
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 00/08 :    0   1
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 01/08 :    0   1
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 02/08 :    0   1
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 03/08 :    0   1
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 04/08 :    0   1
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 05/08 :    0   1
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 06/08 :    0   1
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 07/08 :    0   1
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 00/08 :    0   1
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 01/08 :    0   1
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 02/08 :    0   1
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 03/08 :    0   1
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 04/08 :    0   1
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 05/08 :    0   1
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 06/08 :    0   1
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 07/08 :    0   1
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO Channel 00 : 1[44000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO Channel 00 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 00 : 0[3000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 00 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO Channel 01 : 1[44000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 01 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO Channel 01 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 01 : 0[3000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO Channel 02 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO Channel 02 : 1[44000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 02 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 02 : 0[3000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 03 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO Channel 03 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO Channel 03 : 1[44000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO Channel 04 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 03 : 0[3000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 04 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO Channel 05 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO Channel 04 : 1[44000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO Channel 06 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 04 : 0[3000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 05 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO Channel 05 : 1[44000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 06 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO Channel 07 : 1[c4000] -> 0[84000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 05 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO Channel 06 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 06 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO Channel 07 : 1[44000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Channel 07 : 0[84000] -> 1[c4000] via P2P/IPC/read
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Channel 07 : 0[3000] -> 1[44000] via P2P/IPC/read
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Connected all rings
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO Connected all trees
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO Connected all rings
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO Connected all trees
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Connected all rings
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO Connected all trees
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default3]:n2gpu1217:40365:40692 [3] NCCL INFO comm 0x1492ac0090d0 rank 1 nranks 2 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO Connected all rings
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO Connected all trees
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO 8 coll channels, 8 p2p channels, 8 p2p channels per peer
[default2]:n2gpu1217:40364:40691 [2] NCCL INFO comm 0x148dc00090d0 rank 0 nranks 2 cudaDev 2 busId 84000 - Init COMPLETE
[default2]:n2gpu1217:40364:40364 [2] NCCL INFO Launch mode Parallel
[default0]:n2gpu1217:40362:40688 [0] NCCL INFO comm 0x1505700090d0 rank 0 nranks 2 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1217:40362:40362 [0] NCCL INFO Launch mode Parallel
[default1]:n2gpu1217:40363:40689 [1] NCCL INFO comm 0x1494bc0090d0 rank 1 nranks 2 cudaDev 1 busId 44000 - Init COMPLETE
[default3]:time (ms) | model-and-optimizer-setup: 11426.57 | train/valid/test-data-iterators-setup: 6463.82
[default0]:[after dataloaders are built] datetime: 2023-04-22 20:43:39 
[default0]:done with setup ...
[default0]:training ...
[default0]:Number of parameters: [tensor rank - pipeline rank] w/ and w/o embeddings:
[default0]:[000-000] 0.3562B / 0.3025B
[default1]:[001-000] 0.3562B / 0.3025B
[default0]:[before the start of training step] datetime: 2023-04-22 20:43:40 
[default0]:[2023-04-22 20:43:40,876] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[default0]:[2023-04-22 20:43:40,876] [INFO] [checkpointing.py:530:forward] ----Partition Activations True, CPU CHECKPOINTING False
[default0]:[2023-04-22 20:43:40,876] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with 24 total layers
[default0]:[2023-04-22 20:43:40,876] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[default0]:[2023-04-22 20:43:40,876] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
[default3]:NCCL version 2.12.12+cuda11.7
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Connected all rings
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO Connected all trees
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Connected all rings
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO Connected all trees
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1217:40363:41560 [1] NCCL INFO comm 0x14944c0090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:[Rank 1] (after 1 iterations) memory (MB) | allocated: 1560.87158203125 | max allocated: 2252.81982421875 | reserved: 2624.0 | max reserved: 2624.0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Connected all rings
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO Connected all trees
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1217:40365:41564 [3] NCCL INFO comm 0x14923c0090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default3]: iteration        1/    7812 | consumed samples:           64 | consumed tokens:        65536 | elapsed time per iteration (s): 157.68 | learning rate: 1.000E-04 | global batch size:    64 | lm loss: 1.104426E+01 | loss scale: 4096.0 | grad norm: 23.146 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.406 | TFLOPs: 0.33 |
[default0]:n2gpu1217:40362:41563 [0] NCCL INFO comm 0x1504c80090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default2]:n2gpu1217:40364:41566 [2] NCCL INFO Connected all rings
[default2]:n2gpu1217:40364:41566 [2] NCCL INFO Connected all trees
[default2]:n2gpu1217:40364:41566 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1217:40364:41566 [2] NCCL INFO comm 0x148d280090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:[Rank 0] (after 1 iterations) memory (MB) | allocated: 1560.87158203125 | max allocated: 2252.81982421875 | reserved: 2624.0 | max reserved: 2624.0
[default3]: iteration        7/    7812 | consumed samples:          448 | consumed tokens:       458752 | elapsed time per iteration (s): 148.85 | learning rate: 1.000E-04 | global batch size:    64 | lm loss: 9.187574E+00 | loss scale: 4096.0 | grad norm: 1.617 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.430 | TFLOPs: 0.34 |
[default3]: iteration       17/    7812 | consumed samples:         1088 | consumed tokens:      1114112 | elapsed time per iteration (s): 149.69 | learning rate: 1.000E-04 | global batch size:    64 | lm loss: 8.296946E+00 | loss scale: 4096.0 | grad norm: 1.112 | num zeros: 0.0 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.428 | TFLOPs: 0.34 |
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 3410114 ON n2gpu1217 CANCELLED AT 2023-04-22T21:27:03 ***
slurmstepd: error: *** STEP 3410114.0 ON n2gpu1217 CANCELLED AT 2023-04-22T21:27:03 ***
